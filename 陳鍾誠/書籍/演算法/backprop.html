<!DOCTYPE html>
  <html>
  <head>
  <meta charset="UTF-8">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.2/katex.min.css">
  <link rel="stylesheet" type="text/css" href="https://ccc-js.github.io/pp6/doc/main.css">
  <!-- 
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.8/styles/atom-one-light.min.css">
  <link rel="stylesheet" type="text/css" href="file:///D:/ccc/js/pp6/doc/main.css"> 
  -->
  </head>
  <body>
  <title></title>
  <header>
    <div style="float:left"><label class="toggle" onclick="toggleSidebar()">≡</label>&nbsp;&nbsp;</div>
    <div style="float:left"><p><a href="../../../陳鍾誠.html" alt="">陳鍾誠</a> / <a href="../../書籍.html" alt="">書籍</a> / <a href="../演算法.html" alt="">演算法</a></p>
</div>
  </header>
  <aside>
  <div>
  <h2> 演算法</h2>

<ol>
    <li><a href="01-tableLookup.html" alt="">查表法</a></li>
    <li><a href="02-random.html" alt="">亂數產生法</a></li>
    <li><a href="03-monteCarlo.html" alt="">蒙地卡羅法</a></li>
    <li><a href="04-iterative.html" alt="">迭代法</a></li>
    <li><a href="05-dynamicProgramming.html" alt="">動態規劃法</a></li>
    <li><a href="06-divideConquer.html" alt="">分割擊破法</a></li>
    <li><a href="07-hashing.html" alt="">雜湊法</a></li>
    <li><a href="08-bruteForce.html" alt="">暴力法</a></li>
    <li><a href="09-numerical.html" alt="">數值算法</a></li>
    <li><a href="10-greedy.html" alt="">貪婪法</a></li>
    <li><a href="11-graph.html" alt="">圖形表示法</a></li>
    <li><a href="12-transformDomain.html" alt="">轉換領域法</a></li>
    <li><a href="13-stringMatching.html" alt="">字串比對法</a></li>
    <li><a href="14-cryptography.html" alt="">密碼學算法</a></li>
    <li><a href="15-reduction.html" alt="">轉換化約法</a></li>
</ol>
<p><!--</p>
<h2> AI 算法</h2>

<ol>
    <li><a href="02-hillClimbing.html" alt="">爬山演算法</a></li>
    <li><a href="https://cccbook.github.io/algjs/docs/html/03-gradientDescendent.html" alt="">梯度下降法</a></li>
    <li><a href="https://cccbook.github.io/algjs/docs/html/04-backPropagation.html" alt="">反傳遞算法</a></li>
    <li><a href="05-geneticAlgorithm.html" alt="">遺傳演算法</a></li>
    <li><a href="16-search.html" alt="">搜尋法</a></li>
    <li><a href="17-gameSearch.html" alt="">對局搜尋法</a></li>
    <li><a href="21-slotFilling.html" alt="">框架填充法</a></li>
</ol>
<h2> 系統程式</h2>

<ol>
    <li><a href="19-recursiveDescendent.html" alt="">遞迴下降法</a></li>
</ol>
<p>--></p>

<h2> 參考資源</h2>

<ul>
    <li><a href="project.html" alt="">參考專案</a></li>
    <li><a href="quantum.html" alt="">量子電腦</a></li>
    <li><a href="parallel.html" alt="">平行算法</a></li>
</ul>
  </div>
  </aside>
  <article>
  <div class="header">
    
    
    
  </div>
  <h1> 反傳遞算法</h1>

<p>如前文所述，《梯度下降法》是《深度學習神經網路》背後的學習算法，但是再輸入變數很多時，純粹靠《多次前向計算》的《梯度下降法》速度會過慢，因此我們需要使用《反傳遞算法》更有效率的計算《梯度》。</p>
<p></p>

<h2> 節點 Node</h2>

<p>反傳遞算法是用來快速計算神經網路中梯度的方法，方法是把每個變數都變成一個具有(值 v + 梯度 g) 的節點 (node)，其結構如下：</p>

<ul>
    <li><a href="https://github.com/cccbook/aijs/blob/master/project/nn1/lib/node.js">https://github.com/cccbook/aijs/blob/master/project/nn1/lib/node.js</a></li>
</ul>
<pre class="code"><code class="js">class Node {
  constructor(v = 0, g = 0) {
    this.v = v // 輸出值 (f(x))
    this.g = g // 梯度值 (偏微分)
  }
}
</code></pre>

<h2> 閘 Gate</h2>

<p>神經網路中的每個運算，都被表示成一種閘 (Gate)，這些閘可以進行正向 forward 計算，也可以反向 backward 傳遞梯度，以下是 Gate 的類別定義。</p>

<ul>
    <li><a href="https://github.com/cccbook/aijs/blob/master/project/nn1/lib/gate.js">https://github.com/cccbook/aijs/blob/master/project/nn1/lib/gate.js</a></li>
</ul>
<pre class="code"><code class="js">class Gate {
  constructor(o, x, y, f, gfx, gfy) {
    this.p = {o:o, x:x, y:y, f:f, gfx:gfx, gfy:gfy||gfx} // 當 gfy 沒設定時，就代表 gfy = gfx
  }

  forward() {
    let p = this.p
    p.o.v = p.f(p.x.v, p.y.v)
  }

  backward() {
    let p = this.p
    p.x.g = p.gfx(p.o.v)
    p.y.g = p.gfy(p.o.v)
  }
}

</code></pre>

<p>對於每個 gate ，函數 f 是用來進行《正向傳遞》forward() 計算的。</p>

<p>舉例而言，我們可以使用以下指令定義一個乘法閘 gmul </p>

<pre class="code"><code class="js">//                                f          gfx       gfy
let gmul = new Gate(o, x, y, (x,y)=&gt;x*y, (x,y)=&gt;y, (x,y)=&gt;x))
</code></pre>

<p>其中的 f(x,y)=>x<strong>y 是 gmul 的正向計算函數，會計算出 o.v = x.v </strong> y.v 的結果，放在節點 o 中。</p>

<p>而 gfx(x,y)=>y 與 gfy(x,y)=>x 是 gmul 的反向計算函數，會根據 o 節點的《值與梯度》 (o.v, o.g) 反向計算出 x, y 的梯度 x.g 與 y.g 。</p>

<p>前向計算函數 f 是個乘法閘 f(x,y)=x*y，應該很容易理解。但是反向計算函數 gfx 與 gfy 為何是 gfx(x,y)=>y 與 gfx(x,y)=>x 呢？</p>

<h2> 反傳遞的原理</h2>

<p>梯度的反向傳遞原理在 Andrej Karpathy 的  <a href="http://karpathy.github.io/neuralnets/" alt="">Hacker's guide to Neural Networks</a> 一文中有詳盡的說明，讓我們在此簡述其精要。</p>

<p>對於 f(x,y) = xy 而言，假如 f 的梯度為 f.g ，那麼由於：</p>

<p><span class="math inline">\frac{\partial{f(x,y)}}{\partial{x}} = \frac{\partial{xy}}{\partial{x}} = y</span></p>

<p>因此 x 的梯度應該是 x.g = y * f.g。 </p>

<p>同理、 y 的梯度 y.g = x * f.g 。</p>

<p>所以只需要知道 f.g 與 f(x,y)=xy，就可以逆向推出 x.g 與 y.g。</p>

<p>這就是所謂的《梯度反傳遞》。</p>

<p>而這也是為何上述程式中 gfx(x,y)=>y 與 gfy(x,y)=>x 的原因。</p>

<p>讓我們考慮一個更複雜的兩層式網路如下圖，該網路是計算 f = (x+y) * z 這個算式。</p>

<p><figure>
  <img src="https://cccbook.github.io/aijs/docs/img/gateNet.png" alt=""></img>
<figcaption></figcaption></figure>
</p>

<p>其中的 q = x+y, 而 f = q*z。</p>

<p>根據偏微分的鏈鎖規則，我們可以用以下數學式描述 f, q, x 之間的梯度關係。</p>

<p><span class="math inline">\frac{\partial{f(q,z)}}{\partial{x}} = \frac{\partial{q(x,y)}}{\partial{x}} \frac{\partial{f(q,z)}}{\partial{q}}</span></p>
<p></p>

<p>由於 f 為輸出，我們可以先將 f 的梯度 f.g 設定為 1，那麼就可以推斷其他變數的梯度值。 </p>

<p>(註：其實輸出 f.g 可以設為一的非零值，代表梯度下降的移動長度，對於有《樣本與標準答案》的學習，可以將 f.g 設為輸出與標準答案的差距)</p>
<p></p>

<p><span class="math inline">\frac{\partial{f(q,z)}}{\partial{q}} = \frac{\partial{qz}}{\partial{q}} =z</span></p>

<p><span class="math inline">x.g = \frac{\partial{f(q,z)}}{\partial{x}} = \frac{\partial{f(q,z)}}{\partial{q}} \frac{\partial{q(x,y)}}{\partial{x}}=\frac{\partial{qz}}{\partial{q}} \frac{\partial{x+y}}{\partial{x}}=z*1</span></p>

<p>因此當 <code>f.g</code> 已知的時候，我們可以算出 <code>q.g</code> ，由於 <code>f = q*z</code> ，因此  q 的梯度 <code>q.g = z*f.g</code>  (當 f.g = 1 時，則 q.g = z)。</p>

<p>接著我們可以透過 <code>q=x+y</code> 來計算 x 的梯度，得到 <code>x.g=q.g*f.g</code> 來計算 x 的梯度 <code>x.g</code> ，於是同樣得到 <code>x.g=z*f.g</code> (因為 <span class="math inline">\frac{\partial{q}}{\partial{x}}=\frac{\partial{x+y}}{\partial{x}}=1</span> )。</p>

<p>經由這種反傳遞的方式，我們就能從輸出端一路到推回中間層、然後再推回輸入端，計算出每個節點的梯度。</p>

<h2> 網路 Net</h2>

<p>透過這些 Node + Gate 的組合，我們可以設計出能計算任何函數的神經網路 Net。</p>

<p>舉例而言，以下網路 net 會計算 <span class="math inline">x^2+y^2</span> ，其輸出結果會放在節點 o 當中。</p>

<ul>
    <li><a href="https://github.com/cccbook/aijs/blob/master/project/nn1/example/02-backprop/f.js">https://github.com/cccbook/aijs/blob/master/project/nn1/example/02-backprop/f.js</a></li>
</ul>
<pre class="code"><code class="js">const net = new nn.Net()

let x = net.variable(2)
let y = net.variable(1)
let x2 = net.mul(x, x)
let y2 = net.mul(y, y)
let o  = net.add(x2, y2)
</code></pre>

<p>我們只要呼叫網路的正向傳遞函數 net.forward() 就能從輸入值開始，經過每個閘的前向傳遞 gate.forward() 計算出結果 。</p>

<ul>
    <li><a href="https://github.com/cccbook/aijs/blob/master/project/nn1/lib/net.js">https://github.com/cccbook/aijs/blob/master/project/nn1/lib/net.js</a></li>
</ul>
<pre class="code"><code class="js">class Net {

  constructor () {
    this.gates = []
  }

  variable (v, g) {
    return new Node(v, g)
  }

  op (x, y, f, gfx, gfy) {
    let o = new Node()
    let g = new Gate(o, x, y, f, gfx, gfy)
    this.gates.push(g)
    this.o = o
    return o
  }

  add (x, y) { return this.op(x, y, (x,y)=&gt;x+y, (x,y)=&gt;1) }
  mul (x, y) { return this.op(x, y, (x,y)=&gt;x*y, (x,y)=&gt;y, (x,y)=&gt;x) }

  forward() { // 正向傳遞計算結果
    for (let gate of this.gates) {
      gate.forward()
    }
    return this.o
  }

  backward() { // 反向傳遞計算梯度
    this.o.g = 1 // 設定輸出節點 o 的梯度為 1
    for (let i=this.gates.length-1; i&gt;=0; i--) { // 反向傳遞計算每個節點 Node 的梯度 g
      let gate = this.gates[i]
      gate.backward()
    }
  }

  watch (nodes) {
    this.nodes = nodes
  }

  dump() {
    return this.nodes
  }
}

</code></pre>

<p>當我們想知道該網路的輸入 (x,y) 應該調整多少，才能朝梯度方向前進時，我們只要呼叫 net.backward() ，就能透過反向傳遞計算出每個節點的梯度。</p>

<h2> 程式執行</h2>

<p>有了上述概念後，讓我們來看個範例好了，以下這個網路是計算 <span class="math inline">o=x^2+y^2</span> 的函數，我們希望能透過《梯度下降+反傳遞算法》找出其最低點。</p>

<ul>
    <li><a href="https://github.com/cccbook/aijs/blob/master/project/nn1/example/02-backprop/f.js">https://github.com/cccbook/aijs/blob/master/project/nn1/example/02-backprop/f.js</a></li>
</ul>
<p>檔案: f.js</p>

<pre class="code"><code class="js">const nn = require('../../nn')
const net = new nn.Net()

let x = net.variable(2)
let y = net.variable(1)
let x2 = net.mul(x, x)
let y2 = net.mul(y, y)
let o  = net.add(x2, y2)

net.watch({x,y,x2,y2,o})

module.exports = new nn.FNet(net, {x:x, y:y})
</code></pre>

<p>該程式的主程式很簡單，只是對該網路呼叫梯度下降法進行優化而已。</p>

<p>檔案: optimizeEx.js</p>

<ul>
    <li><a href="https://github.com/cccbook/aijs/blob/master/project/nn1/example/02-backprop/optimizeEx.js">https://github.com/cccbook/aijs/blob/master/project/nn1/example/02-backprop/optimizeEx.js</a></li>
</ul>
<pre class="code"><code class="js">const nn = require('../../nn')
const f = require('./f')

nn.optimize(f, {x:1, y:1})
</code></pre>

<p>執行的結果如下：</p>
<p></p>

<pre class="code"><code class="">$ cd example/02-backprop

$ node .\gradientEx

p= {x:2.0000, y:1.0000} f(p)= 5
  gp= { x: 4, y: 1 }
p= {x:1.9600, y:0.9900} f(p)= 4.8217
  gp= { x: 3.8415999999999997, y: 0.9801 }
p= {x:1.9216, y:0.9802} f(p)= 4.653275148657
  gp= { x: 3.692485069056, y: 0.960790079601 }
p= {x:1.8847, y:0.9706} f(p)= 4.493987190929792
  gp= { x: 3.5519401090757823, y: 0.9420470818540095 }
// ... 中間省略 ....
p= {x:0.0479, y:0.0467} f(p)= 0.004475389263812065
  gp= { x: 0.00228988872537307, y: 0.0021855005384389947 }
p= {x:0.0478, y:0.0467} f(p)= 0.004471155300865204
  gp= { x: 0.002287697698821976, y: 0.0021834576020432284 }
p= {x:0.0478, y:0.0467} f(p)= 0.004466927345179781
  gp= { x: 0.002285509815916863, y: 0.002181417529262918 }
</code></pre>

<p>上述程式最後找到了 p={x:0.0478, y:0.0467} 這個非常接近 (0,0) 的點，正確地找到了函數 <span class="math inline">x^2+y^2</span> 的最低點 f(0,0)=0。</p>

<p>現在、您應該已經可以理解《梯度下降法與反傳遞算法》的運作原理了。</p>

<p>雖然《反傳遞算法》早在 1963 年就由 Vapnik 提出了，但是卻直到 1986 年時才由於 Hinton 等人在語音辨識上的實作成果而引發注意。</p>

<p>接著《神經網路》又沉寂了十幾年，到了 2011 年 Hinton 的學生用 CNN 捲積神經網路在影像辨識大賽中用 GPU 加速而大勝，再次導致神經網路進化為深度學習而爆紅。</p>

<p>而這一切背後運作的《反傳遞算法》，經歷了整整一甲子，並沒有多大的改變，只要您理解了這個算法的原理，就能輕鬆踏進《深度學習的神經網路領域》了！</p>

  <div class="reference">
  
  </div>
  </article>
  <footer><p><a href="http://www.nqu.edu.tw/educsie/index.php?act=blog&code=list&ids=4" alt="">陳鍾誠</a> 於 <a href="http://www.nqu.edu.tw/" alt="">金門大學</a> <a href="http://www.nqu.edu.tw/educsie/index.php" alt="">資訊工程系</a> -- 本書衍生自 <a href="https://www.wikipedia.org/" alt="">維基百科</a> ，採用 <a href="https://zh.wikipedia.org/zh-hant/Wikipedia%3ACC_BY-SA_3.0%E5%8D%8F%E8%AE%AE%E6%96%87%E6%9C%AC" alt="">CC: BY-SA</a> 授權</p>
</footer>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.2/katex.min.js"></script>
  <script src="https://ccc-js.github.io/pp6/doc/main.js"></script>
  <!--
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.8/highlight.min.js"></script>
  <script src="file:///D:/ccc/js/pp6/doc/main.js"></script>
  -->
  </script>
  </body>
  </html>
  